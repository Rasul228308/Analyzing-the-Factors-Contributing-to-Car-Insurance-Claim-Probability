{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import micropip\nawait micropip.install('seaborn')\nawait micropip.install('missingno')\nawait micropip.install('scipy')\nawait micropip.install('optuna')\nawait micropip.install('matplotlib')\n\nimport pandas as pd\nimport joblib\nimport missingno as msno\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import chi2_contingency\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.isotonic import IsotonicRegression\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport warnings\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nfrom sklearn.metrics import (\n    roc_curve, auc, precision_recall_curve, average_precision_score,\n    confusion_matrix, classification_report, roc_auc_score,\n    brier_score_loss, log_loss\n)\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score\n\nwarnings.filterwarnings('ignore')\n\nplt.style.use('ggplot')\nmpl.rcParams['figure.figsize'] = (12, 8)\npd.options.mode.chained_assignment = None",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 108
    },
    {
      "cell_type": "code",
      "source": "def load_data(filepath, make_copy=True):\n    df = pd.read_csv(filepath)\n    df_copy = df.copy() if make_copy else None\n    return df, df_copy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "def display_basic_info(df):\n    print(\"=== DATASET INFO ===\")\n    df.info()\n    print(\"\\n=== FIRST 5 ROWS ===\")\n    display(df.head())\n    print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n    display(df.describe())\n    return df.info(), df.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "def analyze_missing_data(df, missing_cols=['CREDIT_SCORE', 'ANNUAL_MILEAGE']):\n    print(\"=== MISSING DATA REPORT ===\")\n    for col in missing_cols:\n        missing_count = df[col].isnull().sum()\n        missing_pct = 100 * missing_count / len(df)\n        print(f\"{col}: {missing_pct:.2f}% ({missing_count}/{len(df)})\")\n    return {col: df[col].isnull().sum() for col in missing_cols}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "def analyze_missingness_mechanism(\n    df,\n    target_vars=['CREDIT_SCORE', 'ANNUAL_MILEAGE'],\n    predictor_vars=['VEHICLE_YEAR', 'DRIVING_EXPERIENCE', 'VEHICLE_TYPE']\n):\n    results = []\n    for target in target_vars:\n        for predictor in predictor_vars:\n            contingency = pd.crosstab(df[predictor], df[target].isnull())\n            chi2, p_value, dof, expected = chi2_contingency(contingency)\n            results.append({\n                'Target': target,\n                'Predictor': predictor,\n                'Chi2': chi2,\n                'p-value': p_value,\n                'Mechanism': 'MAR' if p_value < 0.05 else 'MCAR'\n            })\n    return pd.DataFrame(results)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "def plot_chi2_heatmap(summary_df, figsize=(10, 6)):\n    pivot = summary_df.pivot(index='Predictor', columns='Target', values='p-value')\n    plt.figure(figsize=figsize)\n    sns.heatmap(pivot, annot=True, cmap='coolwarm', fmt='.4f', cbar_kws={'label': 'p-value'})\n    plt.title('Missingness Mechanism Test (Chi-Square p-values)')\n    plt.tight_layout()\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "def encode_ordinal_features(\n    df,\n    ordinal_mapping=None\n):\n    if ordinal_mapping is None:\n        ordinal_mapping = {\n            'DRIVING_EXPERIENCE': {'0-9y': 0, '10-19y': 1, '20-29y': 2, '30y+': 3},\n            'EDUCATION': {'none': 0, 'high school': 1, 'university': 2},\n            'INCOME': {'poverty': 0, 'working class': 1, 'middle class': 2, 'upper class': 3},\n            'VEHICLE_YEAR': {'before 2015': 0, 'after 2015': 1},\n            'AGE': {'16-25': 0, '26-39': 1, '40-64': 2, '65+': 3}\n        }\n    \n    for col, mapping in ordinal_mapping.items():\n        if col in df.columns:\n            df[col] = df[col].map(mapping).astype('int8')\n    return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "def encode_binary_features(\n    df,\n    binary_vars=['GENDER', 'VEHICLE_TYPE', 'VEHICLE_OWNERSHIP', 'MARRIED', 'CHILDREN', 'RACE']\n):\n    from sklearn.preprocessing import LabelEncoder\n    for col in binary_vars:\n        if col in df.columns:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str)).astype('int8')\n    return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "def encode_categorical_features(\n    df,\n    cat_vars=['RACE']\n):\n    from sklearn.preprocessing import LabelEncoder\n    for col in cat_vars:\n        if col in df.columns:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str)).astype('int8')\n    return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "def optimize_numeric_dtypes(df):\n    df['ID'] = df['ID'].astype('int32')\n    df['POSTAL_CODE'] = df['POSTAL_CODE'].astype('int32')\n    \n    count_cols = ['SPEEDING_VIOLATIONS', 'DUIS', 'PAST_ACCIDENTS', 'OUTCOME']\n    for col in count_cols:\n        if col in df.columns:\n            df[col] = df[col].astype('int8')\n    \n    float_cols = ['CREDIT_SCORE', 'ANNUAL_MILEAGE']\n    for col in float_cols:\n        if col in df.columns:\n            df[col] = df[col].astype('float32')\n    return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": "def drop_missing_values(\n    df,\n    subset_cols=['CREDIT_SCORE', 'ANNUAL_MILEAGE'],\n    check_outcome=True\n):\n    print(f\"Dataset shape before: {df.shape}\")\n    print(f\"Nulls before:\\n{df[subset_cols].isnull().sum()}\")\n    \n    df_clean = df.dropna(subset=subset_cols).copy()\n    \n    dropped = len(df) - len(df_clean)\n    print(f\"\\nRecords dropped: {dropped:,} ({100*dropped/len(df):.1f}%)\")\n    print(f\"Dataset shape after: {df_clean.shape}\")\n    \n    if check_outcome and 'OUTCOME' in df.columns:\n        print(\"\\n=== TARGET PRESERVATION ===\")\n        print(f\"Claim rate BEFORE: {df['OUTCOME'].mean():.4f}\")\n        print(f\"Claim rate AFTER:  {df_clean['OUTCOME'].mean():.4f}\")\n        diff = abs(df['OUTCOME'].mean() - df_clean['OUTCOME'].mean())\n        print(f\"Difference: {diff:.4f}\")\n        print(\"‚úÖ Preserved\" if diff < 0.02 else \"‚ö†Ô∏è Shifted\")\n    \n    return df_clean",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": "def test_normality(\n    df,\n    numeric_cols=['CREDIT_SCORE', 'ANNUAL_MILEAGE'],\n    sample_size=5000\n):\n    print(\"=== NORMALITY TESTS (Shapiro-Wilk & Kolmogorov-Smirnov) ===\")\n    results = []\n    \n    for col in numeric_cols:\n        x = df[col].dropna().values.astype(float)\n        \n        x_sample = np.random.choice(x, size=min(sample_size, len(x)), replace=False) if len(x) > sample_size else x\n        sh_stat, sh_p = stats.shapiro(x_sample)\n        \n        x_z = (x - x.mean()) / x.std(ddof=1)\n        ks_stat, ks_p = stats.kstest(x_z, 'norm')\n        \n        results.append({\n            'Variable': col,\n            'Shapiro_W': sh_stat,\n            'Shapiro_p': sh_p,\n            'KS_D': ks_stat,\n            'KS_p': ks_p,\n            'Normality': 'Normal' if sh_p > 0.05 and ks_p > 0.05 else 'Non-normal'\n        })\n        \n        print(f\"\\n{col}\")\n        print(f\"  Shapiro-Wilk: W={sh_stat:.3f}, p={sh_p:.3g} -> {'Normal' if sh_p>0.05 else 'Non-normal'}\")\n        print(f\"  Kolmogorov-Smirnov: D={ks_stat:.3f}, p={ks_p:.3g} -> {'Normal' if ks_p>0.05 else 'Non-normal'}\")\n    \n    return pd.DataFrame(results)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": "def plot_diagnostic_plots(\n    df,\n    numeric_cols=['CREDIT_SCORE', 'ANNUAL_MILEAGE', 'SPEEDING_VIOLATIONS', 'DUIS', 'PAST_ACCIDENTS']\n):\n    for col in numeric_cols:\n        x = df[col].dropna().values.astype(float)\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n        fig.suptitle(col, fontsize=14)\n        \n        sns.histplot(x, kde=True, ax=axes[0], bins=30)\n        axes[0].set_title(\"Histogram + KDE\")\n        \n        stats.probplot(x, dist=\"norm\", plot=axes[1])\n        axes[1].set_title(\"QQ plot vs Normal\")\n        \n        sns.boxplot(x=x, ax=axes[2])\n        axes[2].set_title(\"Boxplot\")\n        \n        plt.tight_layout()\n        plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": "def plot_correlation_heatmap(df, target_col='OUTCOME', figsize=(6, 10)):\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    corr = df[num_cols].corr()\n    \n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        corr[[target_col]].sort_values(target_col, ascending=False),\n        annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt=\".2f\"\n    )\n    plt.title(f\"Correlation of Features with {target_col}\")\n    plt.tight_layout()\n    plt.show()\n    return corr[[target_col]].sort_values(target_col, ascending=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": "def plot_categorical_distributions(\n    df,\n    cat_vars=['GENDER', 'RACE', 'DRIVING_EXPERIENCE', 'EDUCATION', 'INCOME', 'VEHICLE_YEAR', 'VEHICLE_TYPE'],\n    figsize=(5, 5)\n):\n    for col in cat_vars:\n        if col in df.columns:\n            counts = df[col].value_counts()\n            plt.figure(figsize=figsize)\n            counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n            plt.ylabel('')\n            plt.title(f'{col} Distribution')\n            plt.tight_layout()\n            plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": "def split_data(\n    df,\n    target_col='OUTCOME',\n    test_size=0.2,\n    val_size=0.1,\n    random_state=42\n):\n    from sklearn.model_selection import train_test_split\n    \n    X = df.drop(columns=[target_col, 'ID'], errors='ignore')\n    y = df[target_col]\n    \n    X_train, X_temp, y_train, y_temp = train_test_split(\n        X, y, test_size=test_size + val_size, random_state=random_state, stratify=y\n    )\n    \n    val_ratio = val_size / (test_size + val_size)\n    X_test, X_val, y_test, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_ratio, random_state=random_state, stratify=y_temp\n    )\n    \n    print(f\"Train: {X_train.shape}, Test: {X_test.shape}, Val: {X_val.shape}\")\n    return X_train, X_test, X_val, y_train, y_test, y_val",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": "def train_logit(X_train, y_train, C=1.0, max_iter=1000, random_state=42):\n    from sklearn.linear_model import LogisticRegression\n    model = LogisticRegression(C=C, max_iter=max_iter, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": "def train_random_forest(\n    X_train, y_train,\n    n_estimators=100,\n    max_depth=10,\n    random_state=42,\n    n_jobs=-1\n):\n    from sklearn.ensemble import RandomForestClassifier\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        random_state=random_state,\n        n_jobs=n_jobs\n    )\n    model.fit(X_train, y_train)\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": "def train_xgboost(X_train, y_train, **kwargs):\n    import xgboost as xgb\n    from xgboost import XGBClassifier\n\n    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n    params = {\n        'n_estimators': 100,\n        'scale_pos_weight': scale_pos_weight,\n        'random_state': 42,\n        'use_label_encoder': False,\n        'eval_metric': 'logloss',\n        'max_depth': 6,\n        'learning_rate': 0.1,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8\n    }\n\n    params.update(kwargs)\n\n    # Use XGBClassifier instead of xgb.train for compatibility\n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    return model\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": "def evaluate_model(model, X_test, y_test, model_name=\"Model\", is_xgb=False, threshold=0.5):\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n    if is_xgb:\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n        y_pred = (y_pred_proba >= threshold).astype(int)  # ADDED: threshold parameter\n    else:\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'Model': model_name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'F1': f1_score(y_test, y_pred),\n        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n    }\n\n    print(f\"\\n=== {model_name} PERFORMANCE ===\")\n    if is_xgb:\n        print(f\"Threshold   : {threshold:.4f}\")  # ADDED: show threshold\n    for k, v in metrics.items():\n        if k != 'Model':\n            print(f\"{k:12}: {v:.4f}\")\n\n    return metrics",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "source": "def compare_models(metrics_list):\n    comparison_df = pd.DataFrame(metrics_list)\n    comparison_df = comparison_df.set_index('Model')\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"MODEL COMPARISON\")\n    print(\"=\"*70)\n    display(comparison_df.round(4))\n    \n    comparison_df.plot(kind='bar', figsize=(12, 6), rot=0)\n    plt.title('Model Performance Comparison')\n    plt.ylabel('Score')\n    plt.legend(loc='lower right')\n    plt.tight_layout()\n    plt.show()\n    \n    return comparison_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": "def optimize_xgboost_optuna(\n    X_train, y_train, X_val=None, y_val=None,\n    n_trials=100,\n    direction='maximize',\n    cv_folds=10,\n    use_cv=True,\n    random_state=42,\n    n_jobs=-1,\n    overfitting_penalty=0.1\n):\n    import optuna\n    import numpy as np\n    from xgboost import XGBClassifier\n    from sklearn.model_selection import cross_val_predict, StratifiedKFold\n    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n            'max_depth': trial.suggest_int('max_depth', 2, 12),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n            'gamma': trial.suggest_float('gamma', 0.01, 15),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n            'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum(),\n            'random_state': random_state,\n            'eval_metric': 'logloss',\n            'use_label_encoder': False\n        }\n\n        threshold = trial.suggest_float('threshold', 0.1, 0.9)\n        model = XGBClassifier(**params)\n\n        if use_cv:\n            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n\n            # Get CV predictions\n            y_proba_cv = cross_val_predict(\n                model, X_train, y_train,\n                cv=cv,\n                method='predict_proba',\n                n_jobs=n_jobs\n            )[:, 1]\n\n            y_pred_cv = (y_proba_cv >= threshold).astype(int)\n\n            # Calculate metrics\n            precision_cv = precision_score(y_train, y_pred_cv, zero_division=0)\n            recall_cv = recall_score(y_train, y_pred_cv, zero_division=0)\n            f1_cv = f1_score(y_train, y_pred_cv, zero_division=0)\n\n            # OVERFITTING CHECK: Train on full training set\n            model.fit(X_train, y_train)\n            y_proba_train = model.predict_proba(X_train)[:, 1]\n            y_pred_train = (y_proba_train >= threshold).astype(int)\n\n            precision_train = precision_score(y_train, y_pred_train, zero_division=0)\n            recall_train = recall_score(y_train, y_pred_train, zero_division=0)\n            f1_train = f1_score(y_train, y_pred_train, zero_division=0)\n\n            # Calculate overfitting gap\n            f1_gap = abs(f1_train - f1_cv)\n            precision_gap = abs(precision_train - precision_cv)\n            recall_gap = abs(recall_train - recall_cv)\n            overfitting_score = (f1_gap + precision_gap + recall_gap) / 3\n\n            # Store diagnostics\n            trial.set_user_attr(\"precision_train\", precision_train)\n            trial.set_user_attr(\"precision_cv\", precision_cv)\n            trial.set_user_attr(\"recall_train\", recall_train)\n            trial.set_user_attr(\"recall_cv\", recall_cv)\n            trial.set_user_attr(\"f1_train\", f1_train)\n            trial.set_user_attr(\"f1_cv\", f1_cv)\n            trial.set_user_attr(\"overfitting_gap\", overfitting_score)\n\n            # OBJECTIVE: Maximize F1, penalize overfitting\n            score = f1_cv - (overfitting_penalty * overfitting_score)\n\n            # Warn about extreme overfitting\n            if overfitting_score > 0.15:\n                trial.set_user_attr(\"warning\", \"‚ö†Ô∏è HIGH OVERFITTING\")\n\n        else:\n            if X_val is None or y_val is None:\n                raise ValueError(\"X_val and y_val must be provided when use_cv=False\")\n\n            model.fit(X_train, y_train)\n\n            # Train metrics\n            y_proba_train = model.predict_proba(X_train)[:, 1]\n            y_pred_train = (y_proba_train >= threshold).astype(int)\n            precision_train = precision_score(y_train, y_pred_train, zero_division=0)\n            recall_train = recall_score(y_train, y_pred_train, zero_division=0)\n            f1_train = f1_score(y_train, y_pred_train, zero_division=0)\n\n            # Validation metrics\n            y_proba_val = model.predict_proba(X_val)[:, 1]\n            y_pred_val = (y_proba_val >= threshold).astype(int)\n            precision_val = precision_score(y_val, y_pred_val, zero_division=0)\n            recall_val = recall_score(y_val, y_pred_val, zero_division=0)\n            f1_val = f1_score(y_val, y_pred_val, zero_division=0)\n\n            # Overfitting check\n            f1_gap = abs(f1_train - f1_val)\n            precision_gap = abs(precision_train - precision_val)\n            recall_gap = abs(recall_train - recall_val)\n            overfitting_score = (f1_gap + precision_gap + recall_gap) / 3\n\n            trial.set_user_attr(\"precision_train\", precision_train)\n            trial.set_user_attr(\"precision_val\", precision_val)\n            trial.set_user_attr(\"recall_train\", recall_train)\n            trial.set_user_attr(\"recall_val\", recall_val)\n            trial.set_user_attr(\"f1_train\", f1_train)\n            trial.set_user_attr(\"f1_val\", f1_val)\n            trial.set_user_attr(\"overfitting_gap\", overfitting_score)\n\n            # OBJECTIVE: Maximize F1, penalize overfitting\n            score = f1_val - (overfitting_penalty * overfitting_score)\n\n            if overfitting_score > 0.15:\n                trial.set_user_attr(\"warning\", \"‚ö†Ô∏è HIGH OVERFITTING\")\n\n        trial.set_user_attr(\"best_threshold\", threshold)\n        return score\n\n    study = optuna.create_study(direction=direction)\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"BEST XGBOOST PARAMETERS (OPTUNA - F1 OPTIMIZED)\")\n    print(\"=\"*70)\n    print(f\"Best F1 Score: {study.best_value:.4f}\")\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"OVERFITTING DIAGNOSTICS\")\n    print(\"=\"*70)\n\n    if use_cv:\n        print(f\"F1 Score (Train): {study.best_trial.user_attrs.get('f1_train', 0):.4f}\")\n        print(f\"F1 Score (CV):    {study.best_trial.user_attrs.get('f1_cv', 0):.4f}\")\n        print(f\"Precision (Train): {study.best_trial.user_attrs.get('precision_train', 0):.4f}\")\n        print(f\"Precision (CV):    {study.best_trial.user_attrs.get('precision_cv', 0):.4f}\")\n        print(f\"Recall (Train):    {study.best_trial.user_attrs.get('recall_train', 0):.4f}\")\n        print(f\"Recall (CV):       {study.best_trial.user_attrs.get('recall_cv', 0):.4f}\")\n    else:\n        print(f\"F1 Score (Train): {study.best_trial.user_attrs.get('f1_train', 0):.4f}\")\n        print(f\"F1 Score (Val):   {study.best_trial.user_attrs.get('f1_val', 0):.4f}\")\n        print(f\"Precision (Train): {study.best_trial.user_attrs.get('precision_train', 0):.4f}\")\n        print(f\"Precision (Val):   {study.best_trial.user_attrs.get('precision_val', 0):.4f}\")\n        print(f\"Recall (Train):    {study.best_trial.user_attrs.get('recall_train', 0):.4f}\")\n        print(f\"Recall (Val):      {study.best_trial.user_attrs.get('recall_val', 0):.4f}\")\n\n    overfitting_gap = study.best_trial.user_attrs.get('overfitting_gap', 0)\n    print(f\"\\nOverfitting Gap:   {overfitting_gap:.4f}\")\n\n    if overfitting_gap < 0.05:\n        print(\"‚úÖ GOOD - No significant overfitting\")\n    elif overfitting_gap < 0.10:\n        print(\"‚ö†Ô∏è  MODERATE - Some overfitting detected\")\n    else:\n        print(\"üö® HIGH - Significant overfitting! Consider:\")\n        print(\"   ‚Ä¢ Increase regularization (reg_alpha, reg_lambda)\")\n        print(\"   ‚Ä¢ Reduce max_depth\")\n        print(\"   ‚Ä¢ Increase min_child_weight\")\n        print(\"   ‚Ä¢ More training data\")\n\n    if study.best_trial.user_attrs.get('warning'):\n        print(f\"\\n{study.best_trial.user_attrs['warning']}\")\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"BEST HYPERPARAMETERS\")\n    print(\"=\"*70)\n    for k, v in study.best_params.items():\n        if k != 'threshold':\n            print(f\"  {k}: {v}\")\n    print(f\"\\nBest Threshold: {study.best_params['threshold']:.4f}\")\n\n    best_params_full = {\n        k: v for k, v in study.best_params.items() if k != 'threshold'\n    }\n    best_params_full.update({\n        'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum(),\n        'random_state': random_state,\n        'eval_metric': 'logloss',\n        'use_label_encoder': False\n    })\n\n    best_threshold = study.best_params['threshold']\n\n    return best_params_full, best_threshold, study",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 58
    },
    {
      "cell_type": "code",
      "source": "def plot_xgboost_diagnostics(model, X_train, y_train, X_test, y_test, model_name=\"XGBoost\"):\n    \"\"\"\n    Comprehensive diagnostic plots for XGBoost classification model\n\n    Parameters:\n    -----------\n    model : trained XGBoost model\n    X_train, y_train : training data\n    X_test, y_test : test data\n    model_name : str, name for plot titles\n    \"\"\"\n\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    y_pred = model.predict(X_test)\n    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n\n    fig = plt.figure(figsize=(20, 12))\n\n    # ==================== PLOT 1: ROC CURVE ====================\n    ax1 = plt.subplot(2, 3, 1)\n    fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n    roc_auc = auc(fpr, tpr)\n\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n\n    optimal_idx = np.argmax(tpr - fpr)\n    optimal_threshold_roc = thresholds_roc[optimal_idx]\n    plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10, \n             label=f'Optimal threshold: {optimal_threshold_roc:.3f}')\n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title(f'{model_name}: ROC Curve', fontsize=14, fontweight='bold')\n    plt.legend(loc=\"lower right\")\n    plt.grid(alpha=0.3)\n\n    # ==================== PLOT 2: PRECISION-RECALL CURVE ====================\n    ax2 = plt.subplot(2, 3, 2)\n    precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n    avg_precision = average_precision_score(y_test, y_pred_proba)\n\n    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.4f})')\n\n    baseline = y_test.sum() / len(y_test)\n    plt.plot([0, 1], [baseline, baseline], 'k--', lw=2, label=f'Baseline ({baseline:.3f})')\n\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n    optimal_idx_pr = np.argmax(f1_scores)\n    plt.plot(recall[optimal_idx_pr], precision[optimal_idx_pr], 'ro', markersize=10,\n             label=f'Best F1={f1_scores[optimal_idx_pr]:.3f} @ t={thresholds_pr[optimal_idx_pr]:.3f}')\n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall', fontsize=12)\n    plt.ylabel('Precision', fontsize=12)\n    plt.title(f'{model_name}: Precision-Recall Curve', fontsize=14, fontweight='bold')\n    plt.legend(loc=\"best\")\n    plt.grid(alpha=0.3)\n\n    # ==================== PLOT 3: CONFUSION MATRIX ====================\n    ax3 = plt.subplot(2, 3, 3)\n    cm = confusion_matrix(y_test, y_pred)\n\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, \n                cbar_kws={'label': 'Count'}, linewidths=2, linecolor='white')\n\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.title(f'{model_name}: Confusion Matrix', fontsize=14, fontweight='bold')\n\n    tn, fp, fn, tp = cm.ravel()\n    plt.text(1.5, -0.3, f'Accuracy: {(tp+tn)/(tp+tn+fp+fn):.3f}\\n'\n                        f'Precision: {tp/(tp+fp):.3f}\\n'\n                        f'Recall: {tp/(tp+fn):.3f}\\n'\n                        f'F1: {2*tp/(2*tp+fp+fn):.3f}',\n             transform=ax3.transAxes, fontsize=10, \n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    # ==================== PLOT 4: FEATURE IMPORTANCE ====================\n    ax4 = plt.subplot(2, 3, 4)\n\n    try:\n        feature_importance = model.feature_importances_\n        if hasattr(X_train, 'columns'):\n            feature_names = X_train.columns\n        else:\n            feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n\n        importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': feature_importance\n        }).sort_values('importance', ascending=True).tail(20)\n\n        plt.barh(range(len(importance_df)), importance_df['importance'], color='steelblue')\n        plt.yticks(range(len(importance_df)), importance_df['feature'])\n        plt.xlabel('Importance Score', fontsize=12)\n        plt.title(f'{model_name}: Top 20 Feature Importances', fontsize=14, fontweight='bold')\n        plt.grid(axis='x', alpha=0.3)\n    except:\n        plt.text(0.5, 0.5, 'Feature importance not available', \n                ha='center', va='center', fontsize=12)\n\n    # ==================== PLOT 5: CALIBRATION CURVE ====================\n    ax5 = plt.subplot(2, 3, 5)\n\n    fraction_of_positives, mean_predicted_value = calibration_curve(\n        y_test, y_pred_proba, n_bins=10, strategy='uniform'\n    )\n\n    plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n             color='red', lw=2, label=f'{model_name}')\n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfect calibration')\n\n    brier = brier_score_loss(y_test, y_pred_proba)\n    plt.text(0.05, 0.95, f'Brier Score: {brier:.4f}', \n             transform=ax5.transAxes, fontsize=10,\n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    plt.xlabel('Mean Predicted Probability', fontsize=12)\n    plt.ylabel('Fraction of Positives', fontsize=12)\n    plt.title(f'{model_name}: Calibration Curve', fontsize=14, fontweight='bold')\n    plt.legend(loc='best')\n    plt.grid(alpha=0.3)\n\n    # ==================== PLOT 6: PREDICTION DISTRIBUTION ====================\n    ax6 = plt.subplot(2, 3, 6)\n\n    plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.6, color='blue', \n             label='Actual Negative', density=True)\n    plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.6, color='red', \n             label='Actual Positive', density=True)\n\n    plt.axvline(0.5, color='black', linestyle='--', lw=2, label='Default threshold (0.5)')\n    plt.axvline(optimal_threshold_roc, color='green', linestyle='--', lw=2, \n                label=f'Optimal threshold ({optimal_threshold_roc:.3f})')\n\n    plt.xlabel('Predicted Probability', fontsize=12)\n    plt.ylabel('Density', fontsize=12)\n    plt.title(f'{model_name}: Prediction Distribution', fontsize=14, fontweight='bold')\n    plt.legend(loc='best')\n    plt.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # ==================== PRINT DIAGNOSTIC SUMMARY ====================\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{model_name.upper()} DIAGNOSTIC SUMMARY\")\n    print(\"=\"*80)\n\n    print(\"\\n1. CLASSIFICATION METRICS:\")\n    print(\"-\" * 80)\n    print(classification_report(y_test, y_pred, digits=4))\n\n    print(\"\\n2. PROBABILITY METRICS:\")\n    print(\"-\" * 80)\n    print(f\"ROC-AUC Score:           {roc_auc:.4f}\")\n    print(f\"Average Precision (AP):  {avg_precision:.4f}\")\n    print(f\"Brier Score:             {brier:.4f} (lower is better)\")\n    print(f\"Log Loss:                {log_loss(y_test, y_pred_proba):.4f} (lower is better)\")\n\n    print(\"\\n3. OPTIMAL THRESHOLDS:\")\n    print(\"-\" * 80)\n    print(f\"ROC-based threshold:     {optimal_threshold_roc:.4f} (max TPR-FPR)\")\n    print(f\"PR-based threshold:      {thresholds_pr[optimal_idx_pr]:.4f} (max F1)\")\n    print(f\"Current threshold:       0.5000 (default)\")\n\n    print(\"\\n4. CLASS DISTRIBUTION:\")\n    print(\"-\" * 80)\n    print(f\"Training set:  Negative={sum(y_train==0)} ({sum(y_train==0)/len(y_train)*100:.1f}%), \"\n          f\"Positive={sum(y_train==1)} ({sum(y_train==1)/len(y_train)*100:.1f}%)\")\n    print(f\"Test set:      Negative={sum(y_test==0)} ({sum(y_test==0)/len(y_test)*100:.1f}%), \"\n          f\"Positive={sum(y_test==1)} ({sum(y_test==1)/len(y_test)*100:.1f}%)\")\n\n    print(\"\\n5. OVERFITTING CHECK:\")\n    print(\"-\" * 80)\n    train_auc = roc_auc_score(y_train, y_train_pred_proba)\n    test_auc = roc_auc\n    print(f\"Training AUC:   {train_auc:.4f}\")\n    print(f\"Test AUC:       {test_auc:.4f}\")\n    print(f\"Difference:     {abs(train_auc - test_auc):.4f}\")\n    if abs(train_auc - test_auc) > 0.05:\n        print(\"‚ö†Ô∏è  WARNING: Possible overfitting (AUC gap > 0.05)\")\n    else:\n        print(\"‚úÖ No significant overfitting detected\")\n\n    return {\n        'roc_auc': roc_auc,\n        'avg_precision': avg_precision,\n        'brier_score': brier,\n        'optimal_threshold_roc': optimal_threshold_roc,\n        'optimal_threshold_pr': thresholds_pr[optimal_idx_pr],\n        'confusion_matrix': cm\n    }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": "def plot_learning_curves(model, X_train, y_train, cv=5, scoring='roc_auc'):\n    \"\"\"\n    Plot learning curves for ANY model (including calibrated wrappers)\n    \"\"\"\n    from sklearn.model_selection import learning_curve\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    # Handle calibrated wrapper vs regular model\n    if hasattr(model, 'get_uncalibrated_proba'):\n        # Use raw XGBoost predictions for learning curves\n        def scorer(estimator, X, y):\n            raw_proba = estimator.xgb_model.predict_proba(X)[:, 1]\n            return raw_proba  # Return probabilities for roc_auc\n    else:\n        # Regular model\n        def scorer(estimator, X, y):\n            return estimator.predict_proba(X)[:, 1]\n    \n    # Calculate learning curves\n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X_train, y_train, \n        cv=cv,\n        scoring=scoring,  # 'roc_auc', 'precision', etc.\n        n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10),\n        random_state=42\n    )\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(12, 8))\n    plt.plot(train_sizes, train_mean, 'o-', color='red', linewidth=2, label='Training AUC', markersize=8)\n    plt.plot(train_sizes, val_mean, 'o-', color='green', linewidth=2, label='CV AUC', markersize=8)\n\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n                     alpha=0.2, color='red')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n                     alpha=0.2, color='green')\n\n    plt.xlabel('Training Set Size (%)', fontsize=14)\n    plt.ylabel(f'{scoring.upper()} Score', fontsize=14)\n    plt.title('Learning Curves: Bias-Variance Diagnosis', fontsize=16, fontweight='bold')\n    plt.legend(fontsize=12)\n    plt.grid(alpha=0.3)\n\n    # Final gap analysis\n    final_gap = train_mean[-1] - val_mean[-1]\n    plt.text(0.02, 0.98, f'Final Gap: {final_gap:.4f}\\\\n'\n                          f'Train: {train_mean[-1]:.4f}\\\\n'\n                          f'CV: {val_mean[-1]:.4f}',\n             transform=plt.gca().transAxes, fontsize=12,\n             verticalalignment='top',\n             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n\n    plt.tight_layout()\n    plt.show()\n\n    # Comprehensive diagnosis\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß† LEARNING CURVE DIAGNOSIS\")\n    print(\"=\"*80)\n    \n    print(f\"Final Train AUC: {train_mean[-1]:.4f}\")\n    print(f\"Final CV AUC:    {val_mean[-1]:.4f}\")\n    print(f\"Gap:            {final_gap:.4f}\")\n    print(f\"CV Std:         {val_scores.std():.4f}\")\n    \n    if final_gap > 0.10:\n        print(\"\\nüö® HIGH VARIANCE (OVERFITTING)\")\n        print(\"   Symptoms:\")\n        print(\"   ‚Ä¢ Large gap (>0.10)\")\n        print(\"   ‚Ä¢ Train >> CV\")\n        print(\"\\n   Fixes:\")\n        print(\"   ‚Ä¢ Increase reg_alpha/reg_lambda\")\n        print(\"   ‚Ä¢ Reduce max_depth\")\n        print(\"   ‚Ä¢ Add dropout (subsample/colsample)\")\n        print(\"   ‚Ä¢ More training data\")\n        \n    elif val_mean[-1] < 0.80:\n        print(\"\\nüö® HIGH BIAS (UNDERFITTING)\")\n        print(\"   Symptoms:\")\n        print(\"   ‚Ä¢ Both curves low (<0.80)\")\n        print(\"   ‚Ä¢ Don't converge to high values\")\n        print(\"\\n   Fixes:\")\n        print(\"   ‚Ä¢ Increase max_depth\")\n        print(\"   ‚Ä¢ Reduce regularization\")\n        print(\"   ‚Ä¢ Add polynomial features\")\n        print(\"   ‚Ä¢ Ensemble multiple models\")\n        \n    elif final_gap < 0.05 and val_mean[-1] > 0.85:\n        print(\"\\n‚úÖ PERFECT - Optimal bias-variance tradeoff!\")\n        print(\"   Ready for production\")\n        \n    else:\n        print(\"\\n‚ö†Ô∏è  MODERATE - Acceptable but room for improvement\")\n        print(\"   Gap reasonable, CV performance good\")\n    \n    print(\"\\nCV Stability: \", end=\"\")\n    if val_scores.std() < 0.02:\n        print(\"‚úÖ EXCELLENT (low variance)\")\n    elif val_scores.std() < 0.05:\n        print(\"‚ö†Ô∏è  GOOD (stable enough)\")\n    else:\n        print(\"üö® POOR (high variance - unstable)\")\n    \n    return {\n        'final_gap': final_gap,\n        'train_auc': train_mean[-1],\n        'cv_auc': val_mean[-1],\n        'cv_std': val_scores.std()\n    }\n\n# Multiple scorings\ndef plot_learning_curves_multi(model, X_train, y_train, cv=5):\n    \"\"\"Plot learning curves for multiple metrics\"\"\"\n    scorings = ['roc_auc', 'precision', 'recall']\n    \n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    for i, scoring in enumerate(scorings):\n        results = plot_learning_curves(model, X_train, y_train, cv=cv, scoring=scoring)\n        plt.close()  # Close individual plot\n        \n        # Plot on subplot (simplified version)\n        train_sizes, train_scores, val_scores = learning_curve(\n            model, X_train, y_train, cv=cv, scoring=scoring,\n            train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n        )\n        axes[i].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Train')\n        axes[i].plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='CV')\n        axes[i].fill_between(train_sizes, \n                           np.mean(train_scores,1)-np.std(train_scores,1),\n                           np.mean(train_scores,1)+np.std(train_scores,1), alpha=0.2)\n        axes[i].fill_between(train_sizes, \n                           np.mean(val_scores,1)-np.std(val_scores,1),\n                           np.mean(val_scores,1)+np.std(val_scores,1), alpha=0.2)\n        axes[i].set_title(f'{scoring.replace(\"_\", \" \").title()}')\n        axes[i].legend()\n        axes[i].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "def plot_threshold_analysis(model, X_test, y_test):\n    \"\"\"\n    Comprehensive threshold tuning analysis\n    \"\"\"\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    thresholds = np.linspace(0.01, 0.99, 100)\n\n    metrics = {\n        'precision': [],\n        'recall': [],\n        'f1': [],\n        'accuracy': [],\n        'fpr': [],\n        'fnr': []\n    }\n\n    for t in thresholds:\n        y_pred_t = (y_pred_proba >= t).astype(int)\n        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_t).ravel()\n\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        accuracy = (tp + tn) / (tp + tn + fp + fn)\n        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n\n        metrics['precision'].append(precision)\n        metrics['recall'].append(recall)\n        metrics['f1'].append(f1)\n        metrics['accuracy'].append(accuracy)\n        metrics['fpr'].append(fpr)\n        metrics['fnr'].append(fnr)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n    # Left plot: Main metrics\n    ax1.plot(thresholds, metrics['precision'], label='Precision', linewidth=2)\n    ax1.plot(thresholds, metrics['recall'], label='Recall', linewidth=2)\n    ax1.plot(thresholds, metrics['f1'], label='F1 Score', linewidth=2)\n    ax1.plot(thresholds, metrics['accuracy'], label='Accuracy', linewidth=2)\n\n    best_f1_idx = np.argmax(metrics['f1'])\n    ax1.axvline(thresholds[best_f1_idx], color='red', linestyle='--', \n                label=f'Best F1 @ {thresholds[best_f1_idx]:.3f}')\n    ax1.axvline(0.5, color='gray', linestyle=':', label='Default (0.5)')\n\n    ax1.set_xlabel('Threshold', fontsize=12)\n    ax1.set_ylabel('Score', fontsize=12)\n    ax1.set_title('Threshold vs Performance Metrics', fontsize=14, fontweight='bold')\n    ax1.legend(loc='best')\n    ax1.grid(alpha=0.3)\n\n    # Right plot: Error rates\n    ax2.plot(thresholds, metrics['fpr'], label='False Positive Rate', linewidth=2, color='red')\n    ax2.plot(thresholds, metrics['fnr'], label='False Negative Rate', linewidth=2, color='blue')\n\n    intersection_idx = np.argmin(np.abs(np.array(metrics['fpr']) - np.array(metrics['fnr'])))\n    ax2.axvline(thresholds[intersection_idx], color='green', linestyle='--',\n                label=f'Equal Error Rate @ {thresholds[intersection_idx]:.3f}')\n\n    ax2.set_xlabel('Threshold', fontsize=12)\n    ax2.set_ylabel('Error Rate', fontsize=12)\n    ax2.set_title('Threshold vs Error Rates', fontsize=14, fontweight='bold')\n    ax2.legend(loc='best')\n    ax2.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"THRESHOLD RECOMMENDATIONS\")\n    print(\"=\"*80)\n    print(f\"Best F1 threshold:      {thresholds[best_f1_idx]:.4f} (F1={metrics['f1'][best_f1_idx]:.4f})\")\n    print(f\"Equal error rate:       {thresholds[intersection_idx]:.4f} (FPR=FNR={metrics['fpr'][intersection_idx]:.4f})\")\n    print(f\"High precision (0.9+):  {thresholds[np.where(np.array(metrics['precision']) >= 0.9)[0][0]]:.4f}\")\n    print(f\"High recall (0.9+):     {thresholds[np.where(np.array(metrics['recall']) >= 0.9)[0][-1]]:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": "def plot_feature_importance(model, feature_names, max_num_features=15, figsize=(10, 8)):\n    import xgboost as xgb\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    xgb.plot_importance(model, ax=ax, max_num_features=max_num_features, importance_type='weight')\n    plt.title('XGBoost Feature Importance')\n    plt.tight_layout()\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": "def diagnose_calibration(model, X_train, y_train, X_test, y_test):\n    \"\"\"Diagnose calibration issues\"\"\"\n    y_train_proba = model.predict_proba(X_train)[:, 1]\n    y_test_proba = model.predict_proba(X_test)[:, 1]\n\n    # Calibration metrics\n    brier_train = brier_score_loss(y_train, y_train_proba)\n    brier_test = brier_score_loss(y_test, y_test_proba)\n    logloss_train = log_loss(y_train, y_train_proba)\n    logloss_test = log_loss(y_test, y_test_proba)\n\n    print(\"CALIBRATION DIAGNOSTICS\")\n    print(\"=\"*50)\n    print(f\"Brier Score - Train: {brier_train:.4f}, Test: {brier_test:.4f}\")\n    print(f\"Log Loss    - Train: {logloss_train:.4f}, Test: {logloss_test:.4f}\")\n    print(f\"Gap (Train-Test): Brier={brier_train-brier_test:.4f}, LogLoss={logloss_train-logloss_test:.4f}\")\n\n    return y_train_proba, y_test_proba",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 67
    },
    {
      "cell_type": "code",
      "source": "def fix_calibration_underestimation(model, X_train, y_train, X_test, y_test, method='isotonic'):\n    \"\"\"\n    Fix underestimation with calibration methods\n    \"\"\"\n\n    print(f\"\\nCALIBRATION FIX: {method.upper()}\")\n    print(\"=\"*50)\n\n    # Method 1: Isotonic Regression (non-parametric, best for underestimation)\n    if method == 'isotonic':\n        iso = IsotonicRegression(out_of_bounds='clip')\n        y_train_proba = model.predict_proba(X_train)[:, 1]\n        iso.fit(y_train_proba, y_train)\n\n        # Calibrate test predictions\n        y_calib_proba = iso.predict(model.predict_proba(X_test)[:, 1])\n\n    # Method 2: Platt Scaling (sigmoid fit)\n    elif method == 'platt':\n        calib_model = CalibratedClassifierCV(\n            model, method='sigmoid', cv='prefit'\n        )\n        calib_model.fit(X_train, y_train)\n        y_calib_proba = calib_model.predict_proba(X_test)[:, 1]\n\n    # Method 3: Beta Calibration (more robust than Platt)\n    elif method == 'beta':\n        from sklearn.calibration import _calibration\n        beta_calib = CalibratedClassifierCV(\n            model, method='sigmoid', cv=5\n        )\n        beta_calib.fit(X_train, y_train)\n        y_calib_proba = beta_calib.predict_proba(X_test)[:, 1]\n\n    # Method 4: Simple probability shift (quick fix)\n    elif method == 'shift':\n        y_train_proba = model.predict_proba(X_train)[:, 1]\n        # Shift probabilities up by 0.1 (adjust based on your calibration curve)\n        shift_amount = 0.08  # Tune this based on your curve\n        y_calib_proba = np.clip(model.predict_proba(X_test)[:, 1] + shift_amount, 0, 1)\n\n    # Evaluate improvement\n    brier_original = brier_score_loss(y_test, model.predict_proba(X_test)[:, 1])\n    brier_calib = brier_score_loss(y_test, y_calib_proba)\n    print(f\"Brier Score Improvement: {brier_original:.4f} ‚Üí {brier_calib:.4f} ({(1-brier_calib/brier_original)*100:.1f}% better)\")\n\n    # Plot before/after\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Original calibration\n    frac_pos_orig, mean_pred_orig = calibration_curve(\n        y_test, model.predict_proba(X_test)[:, 1], n_bins=10\n    )\n    ax1.plot(mean_pred_orig, frac_pos_orig, \"s-\", label=\"Original\")\n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfect\")\n    ax1.set_title(\"Before Calibration\")\n    ax1.legend()\n\n    # Calibrated calibration\n    frac_pos_calib, mean_pred_calib = calibration_curve(y_test, y_calib_proba, n_bins=10)\n    ax2.plot(mean_pred_calib, frac_pos_calib, \"o-\", label=\"Calibrated\")\n    ax2.plot([0, 1], [0, 1], \"k:\", label=\"Perfect\")\n    ax2.set_title(f\"After {method} Calibration\")\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return y_calib_proba",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 68
    },
    {
      "cell_type": "code",
      "source": "def complete_calibration_pipeline(model, X_train, y_train, X_test, y_test):\n    \"\"\"\n    Complete calibration fix with recommendations\n    \"\"\"\n    print(\"COMPLETE CALIBRATION PIPELINE\")\n    print(\"=\"*50)\n\n    # 1. Diagnose\n    diagnose_calibration(model, X_train, y_train, X_test, y_test)\n\n    # 2. Try all methods and pick best\n    methods = ['isotonic', 'platt', 'shift']\n    best_method = None\n    best_brier = float('inf')\n    results = {}\n\n    for method in methods:\n        y_calib = fix_calibration_underestimation(\n            model, X_train, y_train, X_test, y_test, method=method\n        )\n        brier = brier_score_loss(y_test, y_calib)\n        results[method] = brier\n\n        if brier < best_brier:\n            best_brier = brier\n            best_method = method\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"BEST CALIBRATION METHOD:\")\n    print(f\"{best_method.upper()}: Brier = {best_brier:.4f}\")\n\n    # 3. Train final calibrated model\n    if best_method == 'isotonic':\n        final_calib = IsotonicRegression(out_of_bounds='clip')\n        final_calib.fit(model.predict_proba(X_train)[:, 1], y_train)\n        final_y_calib = final_calib.predict(model.predict_proba(X_test)[:, 1])\n\n    elif best_method == 'platt':\n        final_calib = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n        final_calib.fit(X_train, y_train)\n        final_y_calib = final_calib.predict_proba(X_test)[:, 1]\n\n    return final_calib, final_y_calib, results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 89
    },
    {
      "cell_type": "code",
      "source": "def analyze_prediction_errors(model, X_train, X_test, y_test, feature_names=None, threshold=0.549):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    y_proba = model.predict_proba(X_test)[:, 1]\n    y_pred = (y_proba >= threshold).astype(int)\n\n    if feature_names is None:\n        feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(X_test.shape[1])]\n\n    fp_mask = (y_test == 0) & (y_pred == 1)\n    fn_mask = (y_test == 1) & (y_pred == 0)\n    tp_mask = (y_test == 1) & (y_pred == 1)\n    tn_mask = (y_test == 0) & (y_pred == 0)\n\n    print(\"=\"*80)\n    print(\"ERROR ANALYSIS: FALSE POSITIVES & FALSE NEGATIVES\")\n    print(\"=\"*80)\n\n    print(f\"\\nFalse Positives: {fp_mask.sum()} ({fp_mask.sum()/len(y_test)*100:.1f}%)\")\n    print(f\"False Negatives: {fn_mask.sum()} ({fn_mask.sum()/len(y_test)*100:.1f}%)\")\n    print(f\"True Positives:  {tp_mask.sum()} ({tp_mask.sum()/len(y_test)*100:.1f}%)\")\n    print(f\"True Negatives:  {tn_mask.sum()} ({tn_mask.sum()/len(y_test)*100:.1f}%)\")\n\n    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"FALSE POSITIVE ANALYSIS\")\n    print(\"=\"*80)\n    fp_features = X_test_df[fp_mask]\n    print(fp_features.describe().T[['mean', 'std', '50%']])\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"FALSE NEGATIVE ANALYSIS\")\n    print(\"=\"*80)\n    fn_features = X_test_df[fn_mask]\n    tn_features = X_test_df[tn_mask]\n    print(fn_features.describe().T[['mean', 'std', '50%']])\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPARISON: FP vs TP\")\n    print(\"=\"*80)\n    tp_features = X_test_df[tp_mask]\n    comparison = pd.DataFrame({\n        'FP_Mean': fp_features.mean(),\n        'TP_Mean': tp_features.mean(),\n        'Difference': fp_features.mean() - tp_features.mean(),\n        'FP_Std': fp_features.std(),\n        'TP_Std': tp_features.std()\n    }).sort_values('Difference', key=abs, ascending=False)\n    print(comparison.head(10))\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    fig.suptitle('Error Analysis: Feature Distributions', fontsize=16, fontweight='bold')\n\n    top_features = comparison.head(6).index\n\n    for idx, feature in enumerate(top_features):\n        ax = axes[idx // 3, idx % 3]\n\n        ax.hist(X_test_df[tn_mask][feature], bins=20, alpha=0.4, label='TN', color='green')\n        ax.hist(X_test_df[fp_mask][feature], bins=20, alpha=0.4, label='FP', color='red')\n        ax.hist(X_test_df[tp_mask][feature], bins=20, alpha=0.4, label='TP', color='blue')\n        ax.hist(X_test_df[fn_mask][feature], bins=20, alpha=0.4, label='FN', color='orange')\n\n        ax.set_title(f'{feature}', fontweight='bold')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        ax.legend()\n        ax.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"PROBABILITY DISTRIBUTION BY ERROR TYPE\")\n    print(\"=\"*80)\n\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.hist(y_proba[fp_mask], bins=20, alpha=0.7, color='red', edgecolor='black')\n    plt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n    plt.xlabel('Predicted Probability', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.title('False Positives: Probability Distribution', fontweight='bold')\n    plt.legend()\n    plt.grid(alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    plt.hist(y_proba[fn_mask], bins=20, alpha=0.7, color='orange', edgecolor='black')\n    plt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n    plt.xlabel('Predicted Probability', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.title('False Negatives: Probability Distribution', fontweight='bold')\n    plt.legend()\n    plt.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"\\nFP Probability Range: [{y_proba[fp_mask].min():.3f} - {y_proba[fp_mask].max():.3f}]\")\n    print(f\"FN Probability Range: [{y_proba[fn_mask].min():.3f} - {y_proba[fn_mask].max():.3f}]\")\n\n    return {\n        'fp_features': fp_features,\n        'fn_features': fn_features,\n        'tp_features': tp_features,\n        'tn_features': tn_features,\n        'comparison': comparison\n    }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 2: SHAP VALUES - EXPLAINABILITY (CRITICAL!)\n# ============================================================================\n\ndef analyze_shap_values(model, X_train, X_test, y_test):\n    import shap\n    import matplotlib.pyplot as plt\n\n    print(\"Computing SHAP values (may take 1-2 minutes)...\")\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"SHAP ANALYSIS: MODEL EXPLAINABILITY\")\n    print(\"=\"*80)\n\n    print(\"\\n1. GLOBAL FEATURE IMPORTANCE (Mean |SHAP|)\")\n    plt.figure(figsize=(10, 6))\n    shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n    plt.title('SHAP Feature Importance', fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n2. FEATURE IMPACT DIRECTION (Beeswarm Plot)\")\n    plt.figure(figsize=(10, 8))\n    shap.summary_plot(shap_values, X_test, show=False)\n    plt.title('SHAP Summary Plot (Red=High Feature Value, Blue=Low)', fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n3. TOP 3 FEATURE DEPENDENCE PLOTS\")\n\n    if hasattr(X_test, 'columns'):\n        feature_names = X_test.columns\n    else:\n        feature_names = [f'Feature_{i}' for i in range(X_test.shape[1])]\n\n    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n    top_3_idx = np.argsort(mean_abs_shap)[-3:][::-1]\n\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    for i, idx in enumerate(top_3_idx):\n        shap.dependence_plot(\n            idx, shap_values, X_test, \n            ax=axes[i], show=False\n        )\n        axes[i].set_title(f'{feature_names[idx]}', fontweight='bold')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n4. EXAMPLE: WHY DID MODEL FLAG THIS CUSTOMER?\")\n    fp_mask = (y_test == 0) & ((model.predict_proba(X_test)[:, 1] >= 0.549).astype(int) == 1)\n\n    if fp_mask.sum() > 0:\n        fp_idx = np.where(fp_mask)[0][0]\n\n        plt.figure(figsize=(12, 3))\n        shap.force_plot(\n            explainer.expected_value, \n            shap_values[fp_idx], \n            X_test.iloc[fp_idx] if hasattr(X_test, 'iloc') else X_test[fp_idx],\n            matplotlib=True,\n            show=False\n        )\n        plt.title(f'False Positive Explanation (Index {fp_idx})', fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n    return explainer, shap_values",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 93
    },
    {
      "cell_type": "code",
      "source": "def explain_single_prediction(model, X_test, idx, threshold=0.549, top_features=10):\n    \"\"\"\n    FIXED: Manual SHAP-like explanation for single prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    # FIX: Handle both DataFrame and array\n    if hasattr(X_test, 'iloc'):\n        sample = X_test.iloc[idx].values.reshape(1, -1)\n        feature_names = X_test.columns.tolist()\n    else:\n        sample = X_test[idx].reshape(1, -1)\n        feature_names = [f'Feature_{i}' for i in range(X_test.shape[1])]\n    \n    proba = model.predict_proba(sample)[0, 1]\n    pred = 1 if proba >= threshold else 0\n    \n    feature_importance = model.feature_importances_\n    \n    # Sort by importance\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': feature_importance,\n        'value': X_test.iloc[idx] if hasattr(X_test, 'iloc') else X_test[idx]\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"Prediction {idx}: Proba={proba:.3f} ‚Üí {'CLAIM' if pred==1 else 'NO CLAIM'}\")\n    print(\"\\\\nTop 10 Features Driving This Decision:\")\n    print(importance_df.head(10)[['feature', 'importance', 'value']].round(3))\n    \n    # Plot\n    plt.figure(figsize=(10, 8))\n    top10 = importance_df.head(10)\n    colors = ['red' if i < 5 else 'blue' for i in range(10)]\n    plt.barh(range(len(top10)), top10['importance'], color=colors)\n    plt.yticks(range(len(top10)), [f\"{row.feature}\" for _, row in top10.iterrows()])\n    plt.xlabel('Feature Importance')\n    plt.title(f'Top Features for Prediction {idx} (Proba={proba:.3f})')\n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df.head(10)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 111
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 3: BUSINESS METRICS - COST-BENEFIT & RISK STRATIFICATION\n# ============================================================================\n\ndef business_impact_analysis(model, X_test, y_test, threshold=0.549):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import confusion_matrix\n\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    print(\"=\"*80)\n    print(\"BUSINESS IMPACT ANALYSIS\")\n    print(\"=\"*80)\n\n    print(\"\\n1. COST-BENEFIT ANALYSIS\")\n    print(\"-\"*80)\n\n    cost_fp = 150\n    cost_fn = 8000\n    benefit_tp = 5000\n\n    thresholds = np.linspace(0.01, 0.99, 100)\n    net_profit = []\n    precision_list = []\n    recall_list = []\n\n    for t in thresholds:\n        y_pred_t = (y_proba >= t).astype(int)\n        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_t).ravel()\n\n        profit = (tp * benefit_tp) - (fp * cost_fp) - (fn * cost_fn)\n        net_profit.append(profit)\n\n        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n        precision_list.append(prec)\n        recall_list.append(rec)\n\n    optimal_idx = np.argmax(net_profit)\n    optimal_business_threshold = thresholds[optimal_idx]\n    max_profit = net_profit[optimal_idx]\n\n    current_profit_idx = np.argmin(np.abs(thresholds - threshold))\n    current_profit = net_profit[current_profit_idx]\n\n    print(f\"Cost per False Positive:  ${cost_fp:,}\")\n    print(f\"Cost per False Negative:  ${cost_fn:,}\")\n    print(f\"Benefit per True Positive: ${benefit_tp:,}\")\n    print(f\"\\nCurrent Threshold ({threshold}):\")\n    print(f\"  Net Profit: ${current_profit:,.2f}\")\n    print(f\"  Precision: {precision_list[current_profit_idx]:.3f}\")\n    print(f\"  Recall: {recall_list[current_profit_idx]:.3f}\")\n    print(f\"\\nOptimal Business Threshold ({optimal_business_threshold:.3f}):\")\n    print(f\"  Net Profit: ${max_profit:,.2f}\")\n    print(f\"  Improvement: ${max_profit - current_profit:,.2f} ({(max_profit/current_profit - 1)*100:.1f}%)\")\n    print(f\"  Precision: {precision_list[optimal_idx]:.3f}\")\n    print(f\"  Recall: {recall_list[optimal_idx]:.3f}\")\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n    ax1.plot(thresholds, net_profit, linewidth=2, color='green')\n    ax1.axvline(threshold, color='blue', linestyle='--', linewidth=2, \n                label=f'Current ({threshold}): ${current_profit:,.0f}')\n    ax1.axvline(optimal_business_threshold, color='red', linestyle='--', linewidth=2,\n                label=f'Optimal ({optimal_business_threshold:.3f}): ${max_profit:,.0f}')\n    ax1.set_xlabel('Threshold', fontsize=12)\n    ax1.set_ylabel('Net Profit ($)', fontsize=12)\n    ax1.set_title('Cost-Benefit Analysis: Profit vs Threshold', fontweight='bold', fontsize=14)\n    ax1.legend()\n    ax1.grid(alpha=0.3)\n\n    ax2.plot(thresholds, precision_list, label='Precision', linewidth=2, color='blue')\n    ax2.plot(thresholds, recall_list, label='Recall', linewidth=2, color='orange')\n    ax2.axvline(threshold, color='gray', linestyle=':', label=f'Current ({threshold})')\n    ax2.axvline(optimal_business_threshold, color='red', linestyle='--', \n                label=f'Optimal Business ({optimal_business_threshold:.3f})')\n    ax2.set_xlabel('Threshold', fontsize=12)\n    ax2.set_ylabel('Score', fontsize=12)\n    ax2.set_title('Precision/Recall at Business Optimal Threshold', fontweight='bold', fontsize=14)\n    ax2.legend()\n    ax2.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n2. RISK STRATIFICATION\")\n    print(\"-\"*80)\n\n    risk_tiers = pd.qcut(y_proba, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'], duplicates='drop')\n\n    tier_analysis = pd.DataFrame({\n        'Risk_Tier': risk_tiers,\n        'Actual_Claim': y_test\n    }).groupby('Risk_Tier').agg({\n        'Actual_Claim': ['count', 'sum', 'mean']\n    })\n\n    tier_analysis.columns = ['Count', 'Claims', 'Claim_Rate']\n    print(tier_analysis)\n\n    plt.figure(figsize=(10, 6))\n    tier_analysis['Claim_Rate'].plot(kind='bar', color='steelblue', edgecolor='black', linewidth=1.5)\n    plt.xlabel('Risk Tier', fontsize=12)\n    plt.ylabel('Actual Claim Rate', fontsize=12)\n    plt.title('Risk Stratification: Model Performance by Tier', fontweight='bold', fontsize=14)\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n3. PREMIUM PRICING SIMULATION\")\n    print(\"-\"*80)\n\n    base_premium = 1200\n    avg_claim_amount = 8000\n\n    premium = base_premium * (1 + y_proba * 2)\n\n    total_premiums = premium.sum()\n    total_claims = (y_test * avg_claim_amount).sum()\n    profit = total_premiums - total_claims\n    loss_ratio = total_claims / total_premiums\n\n    print(f\"Base Premium:      ${base_premium:,}\")\n    print(f\"Avg Claim Amount:  ${avg_claim_amount:,}\")\n    print(f\"\\nTotal Premiums:    ${total_premiums:,.2f}\")\n    print(f\"Total Claims Paid: ${total_claims:,.2f}\")\n    print(f\"Net Profit:        ${profit:,.2f}\")\n    print(f\"Loss Ratio:        {loss_ratio:.2%} (target: <75%)\")\n\n    if loss_ratio < 0.75:\n        print(\"‚úÖ PROFITABLE - Loss ratio under industry target!\")\n    else:\n        print(\"‚ö†Ô∏è  HIGH LOSS RATIO - Consider premium adjustments\")\n\n    return {\n        'optimal_threshold': optimal_business_threshold,\n        'max_profit': max_profit,\n        'current_profit': current_profit,\n        'tier_analysis': tier_analysis,\n        'loss_ratio': loss_ratio\n    }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 116
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 4: PERMUTATION IMPORTANCE\n# ============================================================================\n\ndef compute_permutation_importance(model, X_test, y_test):\n    from sklearn.inspection import permutation_importance\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    print(\"=\"*80)\n    print(\"PERMUTATION IMPORTANCE (True Predictive Power)\")\n    print(\"=\"*80)\n    print(\"\\nComputing (may take 30-60 seconds)...\")\n\n    perm_importance = permutation_importance(\n        model, X_test, y_test,\n        n_repeats=10,\n        random_state=42,\n        scoring='roc_auc',\n        n_jobs=-1\n    )\n\n    feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(X_test.shape[1])]\n\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': perm_importance.importances_mean,\n        'std': perm_importance.importances_std\n    }).sort_values('importance', ascending=False)\n\n    print(\"\\nTop 15 Most Important Features:\")\n    print(importance_df.head(15).to_string(index=False))\n\n    plt.figure(figsize=(10, 8))\n    top_20 = importance_df.head(20)\n    plt.barh(range(len(top_20)), top_20['importance'], xerr=top_20['std'], \n             color='steelblue', edgecolor='black', linewidth=1)\n    plt.yticks(range(len(top_20)), top_20['feature'])\n    plt.xlabel('Permutation Importance (AUC Drop)', fontsize=12)\n    plt.title('Permutation Importance: Top 20 Features', fontweight='bold', fontsize=14)\n    plt.grid(axis='x', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    return importance_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 95
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 5: BOOTSTRAP CONFIDENCE INTERVALS\n# ============================================================================\n\ndef bootstrap_confidence_intervals(model, X_test, y_test, n_iterations=1000):\n    from sklearn.utils import resample\n    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    print(\"=\"*80)\n    print(\"BOOTSTRAP CONFIDENCE INTERVALS (Uncertainty Estimation)\")\n    print(\"=\"*80)\n    print(f\"\\nRunning {n_iterations} bootstrap iterations...\")\n\n    y_proba = model.predict_proba(X_test)[:, 1]\n    y_pred = (y_proba >= 0.549).astype(int)\n\n    bootstrap_results = {\n        'auc': [],\n        'precision': [],\n        'recall': [],\n        'f1': []\n    }\n\n    for i in range(n_iterations):\n        indices = resample(range(len(y_test)), random_state=i)\n        y_test_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n        y_pred_boot = y_pred[indices]\n        y_proba_boot = y_proba[indices]\n\n        bootstrap_results['auc'].append(roc_auc_score(y_test_boot, y_proba_boot))\n        bootstrap_results['precision'].append(precision_score(y_test_boot, y_pred_boot, zero_division=0))\n        bootstrap_results['recall'].append(recall_score(y_test_boot, y_pred_boot, zero_division=0))\n        bootstrap_results['f1'].append(f1_score(y_test_boot, y_pred_boot, zero_division=0))\n\n    print(\"\\nRESULTS WITH 95% CONFIDENCE INTERVALS:\")\n    print(\"-\"*80)\n\n    for metric, values in bootstrap_results.items():\n        mean_val = np.mean(values)\n        ci_lower = np.percentile(values, 2.5)\n        ci_upper = np.percentile(values, 97.5)\n        print(f\"{metric.upper():12s}: {mean_val:.4f} [95% CI: {ci_lower:.4f} - {ci_upper:.4f}]\")\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.ravel()\n\n    for idx, (metric, values) in enumerate(bootstrap_results.items()):\n        ax = axes[idx]\n        ax.hist(values, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n\n        mean_val = np.mean(values)\n        ci_lower = np.percentile(values, 2.5)\n        ci_upper = np.percentile(values, 97.5)\n\n        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')\n        ax.axvline(ci_lower, color='green', linestyle=':', linewidth=2, label=f'95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n        ax.axvline(ci_upper, color='green', linestyle=':', linewidth=2)\n\n        ax.set_xlabel(metric.upper(), fontsize=12)\n        ax.set_ylabel('Frequency', fontsize=12)\n        ax.set_title(f'Bootstrap Distribution: {metric.upper()}', fontweight='bold')\n        ax.legend()\n        ax.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return bootstrap_results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 96
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 6: CROSS-VALIDATION WITH MULTIPLE METRICS\n# ============================================================================\n\ndef cross_validation_multiple_metrics(model, X_train, y_train, cv=10):\n    from sklearn.model_selection import cross_validate\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    print(\"=\"*80)\n    print(\"CROSS-VALIDATION: MULTIPLE METRICS\")\n    print(\"=\"*80)\n    print(f\"\\nRunning {cv}-fold cross-validation...\")\n\n    scoring = {\n        'roc_auc': 'roc_auc',\n        'accuracy': 'accuracy',\n        'precision': 'precision',\n        'recall': 'recall',\n        'f1': 'f1',\n        'brier': 'neg_brier_score',\n        'log_loss': 'neg_log_loss'\n    }\n\n    cv_results = cross_validate(\n        model, X_train, y_train,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1\n    )\n\n    print(\"\\nRESULTS:\")\n    print(\"-\"*80)\n    print(f\"{'Metric':<15} {'Train Mean':<12} {'Test Mean':<12} {'Test Std':<10} {'Gap':<10}\")\n    print(\"-\"*80)\n\n    results_summary = []\n\n    for metric in scoring.keys():\n        train_mean = cv_results[f'train_{metric}'].mean()\n        test_mean = cv_results[f'test_{metric}'].mean()\n        test_std = cv_results[f'test_{metric}'].std()\n        gap = abs(train_mean - test_mean)\n\n        if metric in ['brier', 'log_loss']:\n            train_mean = -train_mean\n            test_mean = -test_mean\n\n        print(f\"{metric:<15} {train_mean:>11.4f} {test_mean:>11.4f} {test_std:>9.4f} {gap:>9.4f}\")\n\n        results_summary.append({\n            'metric': metric,\n            'train_mean': train_mean,\n            'test_mean': test_mean,\n            'test_std': test_std,\n            'gap': gap\n        })\n\n    results_df = pd.DataFrame(results_summary)\n\n    print(\"\\nOVERFITTING ANALYSIS:\")\n    print(\"-\"*80)\n    high_gap = results_df[results_df['gap'] > 0.05]\n    if len(high_gap) > 0:\n        print(\"‚ö†Ô∏è  Metrics with high train-test gap (>0.05):\")\n        for _, row in high_gap.iterrows():\n            print(f\"  ‚Ä¢ {row['metric']}: gap = {row['gap']:.4f}\")\n    else:\n        print(\"‚úÖ No significant overfitting detected across all metrics\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n    metrics_plot = results_df[~results_df['metric'].isin(['brier', 'log_loss'])]\n\n    ax1 = axes[0]\n    x_pos = range(len(metrics_plot))\n    ax1.bar([i - 0.2 for i in x_pos], metrics_plot['train_mean'], width=0.4, \n            label='Train', color='red', alpha=0.7, edgecolor='black')\n    ax1.bar([i + 0.2 for i in x_pos], metrics_plot['test_mean'], width=0.4,\n            label='Test', color='green', alpha=0.7, edgecolor='black')\n    ax1.errorbar([i + 0.2 for i in x_pos], metrics_plot['test_mean'], \n                 yerr=metrics_plot['test_std'], fmt='none', color='black', capsize=5)\n    ax1.set_xticks(x_pos)\n    ax1.set_xticklabels(metrics_plot['metric'], rotation=45)\n    ax1.set_ylabel('Score', fontsize=12)\n    ax1.set_title('Train vs Test Performance', fontweight='bold', fontsize=14)\n    ax1.legend()\n    ax1.grid(axis='y', alpha=0.3)\n\n    ax2 = axes[1]\n    ax2.bar(range(len(results_df)), results_df['gap'], color='orange', edgecolor='black', alpha=0.7)\n    ax2.axhline(0.05, color='red', linestyle='--', linewidth=2, label='Overfitting threshold')\n    ax2.set_xticks(range(len(results_df)))\n    ax2.set_xticklabels(results_df['metric'], rotation=45)\n    ax2.set_ylabel('Train-Test Gap', fontsize=12)\n    ax2.set_title('Overfitting Analysis (Gap)', fontweight='bold', fontsize=14)\n    ax2.legend()\n    ax2.grid(axis='y', alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return results_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 97
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# FIXED CELL 7: PARTIAL DEPENDENCE PLOTS (Robust Version)\n# ============================================================================\n\ndef plot_partial_dependence_fixed(model, X_test, feature_names=None, top_n=6):\n    from sklearn.inspection import PartialDependenceDisplay\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    print(\"=\"*80)\n    print(\"PARTIAL DEPENDENCE PLOTS (Marginal Feature Effects)\")\n    print(\"=\"*80)\n    \n    # ROBUST FEATURE NAMES HANDLING\n    if feature_names is None:\n        if hasattr(X_test, 'columns'):\n            feature_names = list(X_test.columns)\n        else:\n            feature_names = [f'Feature_{i}' for i in range(X_test.shape[1])]\n    elif isinstance(feature_names, list):\n        feature_names = feature_names\n    else:\n        feature_names = list(feature_names)\n    \n    print(f\"Feature names detected: {len(feature_names)} features\")\n    \n    # Get top features using XGBoost importance\n    try:\n        feature_importance = model.feature_importances_\n        top_features_idx = np.argsort(feature_importance)[-top_n:][::-1]\n        top_features = [feature_names[i] for i in top_features_idx]\n        print(f\"Top {top_n} features by XGBoost importance: {top_features}\")\n    except:\n        # Fallback to first top_n features\n        top_features = feature_names[:top_n]\n        print(f\"Using first {top_n} features: {top_features}\")\n    \n    # Plot\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    axes = axes.ravel()\n    \n    try:\n        PartialDependenceDisplay.from_estimator(\n            model, X_test, top_features,\n            ax=axes,\n            n_cols=3,\n            grid_resolution=20  # Smoother curves\n        )\n        fig.suptitle('Partial Dependence Plots: Top Features', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        print(\"‚úÖ Partial Dependence plots generated successfully!\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Partial dependence failed: {e}\")\n        print(\"Falling back to single feature plot...\")\n        # Fallback to single most important feature\n        try:\n            top_feature_idx = np.argmax(model.feature_importances_)\n            fig, ax = plt.subplots(figsize=(10, 6))\n            PartialDependenceDisplay.from_estimator(\n                model, X_test, [top_feature_idx],\n                ax=[ax]\n            )\n            ax.set_title(f'Most Important Feature: {feature_names[top_feature_idx]}', fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n        except:\n            print(\"‚ùå Could not generate partial dependence plots\")\n    \n    return top_features\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 124
    },
    {
      "cell_type": "code",
      "source": "\ndef logistic_regression_diagnostics(logit_model, X_train, X_test, y_train, y_test, feature_names=None, threshold=0.5):\n    \"\"\"\n    Complete diagnostics for Logistic Regression + comparison table\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import (roc_auc_score, precision_score, recall_score, \n                                f1_score, confusion_matrix, brier_score_loss, \n                                precision_recall_curve, roc_curve)\n    from sklearn.calibration import calibration_curve\n    \n    if feature_names is None:\n        feature_names = X_test.columns.tolist() if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(X_test.shape[1])]\n    \n    # Predictions\n    y_proba = logit_model.predict_proba(X_test)[:, 1]\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    # Metrics\n    metrics = {\n        'roc_auc': roc_auc_score(y_test, y_proba),\n        'precision': precision_score(y_test, y_pred),\n        'recall': recall_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'brier': brier_score_loss(y_test, y_proba)\n    }\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # 5 DIAGNOSTIC PLOTS\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. ROC Curve\n    ax1 = plt.subplot(2, 3, 1)\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    plt.plot(fpr, tpr, linewidth=3, label=f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.3f})')\n    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('1. ROC Curve', fontweight='bold')\n    plt.legend()\n    plt.grid(alpha=0.3)\n    \n    # 2. Precision-Recall Curve\n    ax2 = plt.subplot(2, 3, 2)\n    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n    plt.plot(rec, prec, linewidth=3, color='purple')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('2. Precision-Recall Curve', fontweight='bold')\n    plt.grid(alpha=0.3)\n    \n    # 3. Confusion Matrix\n    ax3 = plt.subplot(2, 3, 3)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3, \n                cbar_kws={'label': 'Count'})\n    ax3.set_xlabel('Predicted')\n    ax3.set_ylabel('Actual')\n    ax3.set_title('3. Confusion Matrix', fontweight='bold')\n    \n    # 4. Calibration Curve\n    ax4 = plt.subplot(2, 3, 4)\n    frac_pos, mean_pred = calibration_curve(y_test, y_proba, n_bins=10)\n    plt.plot(mean_pred, frac_pos, 's-', linewidth=3, color='red', label='Logistic')\n    plt.plot([0, 1], [0, 1], 'k:', linewidth=2, label='Perfect')\n    plt.xlabel('Mean Predicted Probability')\n    plt.ylabel('Fraction of Positives')\n    plt.title(f'4. Calibration (Brier={metrics[\"brier\"]:.3f})', fontweight='bold')\n    plt.legend()\n    plt.grid(alpha=0.3)\n    \n    # 5. Coefficients Plot (Logistic specific!)\n    ax5 = plt.subplot(2, 3, 5)\n    coefs = pd.DataFrame({\n        'feature': feature_names,\n        'coefficient': logit_model.coef_[0],\n        'abs_coef': np.abs(logit_model.coef_[0])\n    }).sort_values('abs_coef', ascending=True)\n    \n    top_15 = coefs.tail(15)\n    colors = ['red' if x < 0 else 'blue' for x in top_15['coefficient']]\n    \n    plt.barh(range(len(top_15)), top_15['coefficient'], color=colors)\n    plt.yticks(range(len(top_15)), top_15['feature'])\n    plt.xlabel('Coefficient')\n    plt.title('5. Logistic Coefficients (Top 15)', fontweight='bold')\n    plt.axvline(0, color='black', linewidth=1)\n    plt.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print metrics table\n    print(\"=\"*80)\n    print(\"LOGISTIC REGRESSION DIAGNOSTICS\")\n    print(\"=\"*80)\n    print(f\"{'Metric':<15} {'Value':<10}\")\n    print(\"-\"*25)\n    for metric, value in metrics.items():\n        print(f\"{metric:<15} {value:<10.4f}\")\n    \n    print(\"\\\\nLOGISTIC COEFFICIENTS SUMMARY:\")\n    print(\"-\"*50)\n    print(coefs.head(10).to_string(index=False))\n    \n    return metrics, coefs",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 127
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# FIXED LOGISTIC RESIDUAL DIAGNOSTICS (Robust Version)\n# ============================================================================\n\ndef logistic_residual_diagnostics_fixed(logit_model, X_train, X_test, y_train, y_test, threshold=0.549):\n    \"\"\"\n    FIXED version - handles all edge cases\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from scipy import stats\n    from sklearn.metrics import roc_auc_score\n    \n    print(\"Running robust residual diagnostics...\")\n    \n    # Ensure numpy arrays for consistency\n    X_train = np.asarray(X_train)\n    X_test = np.asarray(X_test)\n    y_train = np.asarray(y_train).ravel()\n    y_test = np.asarray(y_test).ravel()\n    \n    # Predictions (handle both raw and calibrated models)\n    try:\n        y_train_proba = logit_model.predict_proba(X_train)[:, 1]\n        y_test_proba = logit_model.predict_proba(X_test)[:, 1]\n    except:\n        # Fallback for production wrapper\n        y_train_proba = logit_model.xgb_model.predict_proba(X_train)[:, 1]\n        y_test_proba = logit_model.xgb_model.predict_proba(X_test)[:, 1]\n    \n    # RESIDUALS\n    train_residuals = y_train - y_train_proba\n    test_residuals = y_test - y_test_proba\n    \n    # 5 DIAGNOSTIC PLOTS (FIXED)\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. RESIDUALS VS FITTED (Most Important!)\n    ax1 = plt.subplot(2, 3, 1)\n    plt.scatter(y_train_proba, train_residuals, alpha=0.6, s=20, c='steelblue')\n    plt.axhline(0, color='red', linestyle='--', linewidth=2)\n    plt.axhline(0.1, color='orange', linestyle=':', alpha=0.7)\n    plt.axhline(-0.1, color='orange', linestyle=':', alpha=0.7)\n    plt.xlabel('Fitted Values (Predicted Probability)')\n    plt.ylabel('Residuals (Actual - Predicted)')\n    plt.title('1. Residuals vs Fitted\\n(Random scatter = Good)', fontweight='bold')\n    plt.grid(alpha=0.3)\n    \n    # 2. QQ Plot (Normality)\n    ax2 = plt.subplot(2, 3, 2)\n    stats.probplot(train_residuals, dist=\"norm\", plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('steelblue')\n    ax2.get_lines()[0].set_markeredgecolor('white')\n    ax2.get_lines()[1].set_linewidth(3)\n    ax2.set_title('2. Q-Q Plot\\n(Points near line = Normal)', fontweight='bold')\n    \n    # 3. RESIDUALS HISTOGRAM\n    ax3 = plt.subplot(2, 3, 3)\n    plt.hist(train_residuals, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\n    plt.axvline(0, color='red', linestyle='--', linewidth=2)\n    plt.xlabel('Residuals')\n    plt.ylabel('Frequency')\n    plt.title('3. Residuals Distribution\\n(Centered at 0 = Good)', fontweight='bold')\n    plt.grid(alpha=0.3)\n    \n    # 4. Scale-Location (Heteroscedasticity)\n    ax4 = plt.subplot(2, 3, 4)\n    std_residuals = np.sqrt(np.abs(train_residuals))\n    plt.scatter(y_train_proba, std_residuals, alpha=0.6, s=20, c='green')\n    plt.xlabel('Fitted Values')\n    plt.ylabel('|Sqrt Residuals|')\n    plt.title('4. Scale-Location\\n(Constant spread = Homoscedastic)', fontweight='bold')\n    plt.grid(alpha=0.3)\n    \n    # 5. RESIDUALS vs ORDER (Serial Correlation)\n    ax5 = plt.subplot(2, 3, 5)\n    plt.scatter(range(len(train_residuals)), train_residuals, alpha=0.6, s=20)\n    plt.axhline(0, color='red', linestyle='--', linewidth=2)\n    plt.xlabel('Observation Order')\n    plt.ylabel('Residuals')\n    plt.title('5. Residuals vs Order\\n(Random = No autocorrelation)', fontweight='bold')\n    plt.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.suptitle('Logistic Regression Residual Diagnostics (Fixed)', fontsize=18, fontweight='bold', y=1.02)\n    plt.show()\n    \n    # RESIDUAL STATISTICS\n    print(\"=\"*80)\n    print(\"RESIDUAL DIAGNOSTICS SUMMARY\")\n    print(\"=\"*80)\n    \n    residual_stats = {\n        'Mean Residual': np.mean(train_residuals),\n        'Median Residual': np.median(train_residuals),\n        'Std Residuals': np.std(train_residuals),\n        'Max |Residual|': np.max(np.abs(train_residuals)),\n        'Skewness': stats.skew(train_residuals),\n        'Kurtosis': stats.kurtosis(train_residuals),\n        'KS p-value': stats.kstest(train_residuals, 'norm')[1],\n        'Train AUC': roc_auc_score(y_train, y_train_proba),\n        'Test AUC': roc_auc_score(y_test, y_test_proba)\n    }\n    \n    stats_df = pd.DataFrame([residual_stats]).T.round(4)\n    print(stats_df)\n    \n    print(\"\\\\nINTERPRETATION:\")\n    print(\"-\" * 40)\n    \n    if abs(np.mean(train_residuals)) < 0.01:\n        print(\"‚úÖ MEAN ‚âà 0: No systematic bias\")\n    else:\n        print(f\"‚ö†Ô∏è  MEAN = {np.mean(train_residuals):.4f}: Systematic bias detected\")\n    \n    if stats.kstest(train_residuals, 'norm')[1] > 0.05:\n        print(\"‚úÖ RESIDUALS NORMAL: Good for inference\")\n    else:\n        print(\"‚ö†Ô∏è  NON-NORMAL: Consider robust methods\")\n    \n    print(\"\\\\n\" + \"=\"*80)\n    print(\"‚úÖ ANALYSIS COMPLETE - 5 PLOTS + STATISTICS\")\n    print(\"=\"*80)\n    \n    return {\n        'residuals': train_residuals,\n        'stats': residual_stats,\n        'train_auc': roc_auc_score(y_train, y_train_proba),\n        'test_auc': roc_auc_score(y_test, y_test_proba)\n    }\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 134
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}